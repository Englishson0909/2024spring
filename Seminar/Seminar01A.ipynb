{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Englishson0909/2024spring/blob/main/Seminar/Seminar01A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ± Seminar 01\n",
        "---\n",
        "\n",
        "\n",
        "1. Introduction:\n",
        "+ 1.1 Why statistics?\n",
        "+ 1.2 **Steps of statistical approach (understanding probability distribution)**\n",
        "+ 1.3 Types of data\n",
        "+ 1.4 Software\n",
        "\n",
        "2. Descriptive statistics overview: Coding"
      ],
      "metadata": {
        "id": "Pzmqs9_Hj3oz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distribution? Probability distribution\n",
        "\n",
        "Parametric vs. non-parametric"
      ],
      "metadata": {
        "id": "0rU4isXZTUzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1] Normal Distribution\n",
        "\n",
        "- The normal distribution, also known as the Gaussian distribution, is one of the most important probability distributions. It is symmetric and describes many natural phenomena, such as the heights of people, test scores, etc."
      ],
      "metadata": {
        "id": "qmkE9R9ATvqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Normal distribution\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Generate data for a normal distribution\n",
        "data = np.random.normal(loc=0, scale=1, size=1000)\n",
        "\n",
        "# Plot the histogram\n",
        "plt.hist(data, bins=30, density=True, alpha=0.6, color='g')\n",
        "\n",
        "# Plot the PDF on top of the histogram\n",
        "xmin, xmax = plt.xlim()\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "p = norm.pdf(x, 0, 1)\n",
        "plt.plot(x, p, 'k', linewidth=2)\n",
        "title = \"Normal Distribution with $\\mu$ = 0, $\\sigma$ = 1\"\n",
        "plt.title(title)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cwXTcZmyTzfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2] Uniform Distribution\n",
        "- The uniform distribution has equal probability for all values in its range. It's often used to model situations where each outcome is equally likely."
      ],
      "metadata": {
        "id": "ZNexHHDOUJXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Uniform distribution\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Generate data for a uniform distribution\n",
        "data = np.random.uniform(low=-1, high=1, size=1000)\n",
        "\n",
        "# Plot the histogram\n",
        "plt.hist(data, bins=30, density=True, alpha=0.6, color='b')\n",
        "\n",
        "# Plot the PDF on top of the histogram\n",
        "xmin, xmax = plt.xlim()\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "p = uniform.pdf(x, -1, 2)\n",
        "plt.plot(x, p, 'k', linewidth=2)\n",
        "plt.title(\"Uniform Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ho0o756bULw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [3] Exponential Distribution\n",
        "- The exponential distribution describes the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate."
      ],
      "metadata": {
        "id": "DcP6Wea9UO5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Exponential distribution\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import expon\n",
        "\n",
        "# Generate data for an exponential distribution\n",
        "data = np.random.exponential(scale=1, size=1000)\n",
        "\n",
        "# Plot the histogram\n",
        "plt.hist(data, bins=30, density=True, alpha=0.6, color='r')\n",
        "\n",
        "# Plot the PDF on top of the histogram\n",
        "xmin, xmax = plt.xlim()\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "p = expon.pdf(x, 0, 1)\n",
        "plt.plot(x, p, 'k', linewidth=2)\n",
        "plt.title(\"Exponential Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7E_KAImtUThp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [4] Binomial Distribution\n",
        "+ The binomial distribution models the number of successes in a fixed number of independent trials of a binary experiment. It is parameterized by n (the number of trials) and p (the probability of success on each trial)."
      ],
      "metadata": {
        "id": "ZWBgZVgwUW_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Binomial distribution: e.g., coin flip\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import binom\n",
        "\n",
        "# Generate data for a binomial distribution\n",
        "n, p = 10, 0.5  # number of trials, probability of each trial\n",
        "data = np.random.binomial(n, p, size=1000)\n",
        "\n",
        "# Plot the histogram\n",
        "plt.hist(data, bins=30, density=True, alpha=0.6, color='y')\n",
        "\n",
        "# Calculate the binomial distribution and plot it\n",
        "x = np.arange(0, n+1)\n",
        "p = binom.pmf(x, n, p)\n",
        "plt.plot(x, p, 'bo', ms=8, label='binom pmf')\n",
        "plt.vlines(x, 0, p, colors='b', lw=5, alpha=0.5)\n",
        "plt.title(\"Binomial Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IQpg4uutUbRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example data"
      ],
      "metadata": {
        "id": "db4bqx0Ya5z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To generate fake names for my sample data generation\n",
        "!pip install faker"
      ],
      "metadata": {
        "id": "856YCgsebHGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Generate a sample data: TOEIC score before and after my classes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from faker import Faker\n",
        "\n",
        "# Initialize Faker to generate names\n",
        "fake = Faker()\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "# Generate data\n",
        "N = 30  # number of students\n",
        "significant_improvers = 5  # Students with significant improvement\n",
        "names = [fake.first_name() for _ in range(N)]\n",
        "toeic_scores_before = np.random.normal(loc=500, scale=100, size=N).astype(int)\n",
        "\n",
        "# Adjust scores for students with significant improvement\n",
        "for i in range(significant_improvers):\n",
        "    toeic_scores_before[i] = np.random.randint(200, 300)\n",
        "\n",
        "# Create 'after' scores for the significant improvers\n",
        "toeic_scores_after = np.empty(N, dtype=int)\n",
        "toeic_scores_after[:significant_improvers] = np.random.randint(980, 990, size=significant_improvers)\n",
        "\n",
        "# The rest of the students\n",
        "normal_part = np.random.normal(loc=510, scale=100, size=(N - significant_improvers)).astype(int)\n",
        "uniform_part_indices = np.random.choice(range(significant_improvers, N), size=(N - significant_improvers)//2, replace=False)\n",
        "normal_part_indices = set(range(significant_improvers, N)) - set(uniform_part_indices)\n",
        "\n",
        "# Assigning the rest of the 'after' scores\n",
        "toeic_scores_after[list(normal_part_indices)] = normal_part[list(normal_part_indices) - np.array(significant_improvers)]\n",
        "toeic_scores_after[uniform_part_indices] = np.random.uniform(low=450, high=550, size=(N - significant_improvers)//2).astype(int)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Student_Name': names,\n",
        "    'TOEIC_Score_Before': toeic_scores_before,\n",
        "    'TOEIC_Score_After': toeic_scores_after\n",
        "})\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UhYXSAc0a7R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"toeic.csv\", index=False)"
      ],
      "metadata": {
        "id": "3YK5wYrRSgUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Paired t-test: before and after\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "# Perform a paired t-test\n",
        "t_statistic, p_value = ttest_rel(df['TOEIC_Score_Before'], df['TOEIC_Score_After'])\n",
        "\n",
        "print(f'Paired t-test statistic: {t_statistic}')\n",
        "print(f'Paired t-test p-value: {p_value}')\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"We reject the null hypothesis, suggesting there is a statistically significant difference in the scores before and after the classes.\")\n",
        "else:\n",
        "    print(\"We do not reject the null hypothesis, suggesting there is not a statistically significant difference in the scores before and after the classes.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NdxSiv3bd2nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Shapiro-Wilk test is used to determine whether a dataset is normally distributed. It's particularly useful in the context of assumptions checking for parametric tests that require normality (like a paired t-test)."
      ],
      "metadata": {
        "id": "SRnGiSu0hbmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Distribution test: Shapiro normality test\n",
        "from scipy.stats import shapiro, normaltest\n",
        "\n",
        "# Perform Shapiro-Wilk test for normality\n",
        "shapiro_before = shapiro(df['TOEIC_Score_Before'])\n",
        "shapiro_after = shapiro(df['TOEIC_Score_After'])\n",
        "\n",
        "print(\"Shapiro-Wilk Test:\")\n",
        "print(f\"Before Classes: Statistics={shapiro_before[0]}, p-value={shapiro_before[1]}\")\n",
        "print(f\"After Classes: Statistics={shapiro_after[0]}, p-value={shapiro_after[1]}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iO3cLTrEbmwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-parametric test"
      ],
      "metadata": {
        "id": "PTIJ2ZAOhiC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Wilcoxon Signed-Rank Test\n",
        "from scipy.stats import wilcoxon\n",
        "\n",
        "# Perform Wilcoxon signed-rank test\n",
        "stat, p = wilcoxon(df['TOEIC_Score_Before'], df['TOEIC_Score_After'])\n",
        "\n",
        "print(f'Wilcoxon signed-rank test statistic: {stat}')\n",
        "print(f'P-value: {p}')\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p < alpha:\n",
        "    print(\"Reject the null hypothesis - suggest a significant difference between the two conditions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis - suggest no significant difference between the two conditions.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "g-hYAZx1gx2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **1.3 Types of data**\n",
        "\n",
        "1. Categorical:\n",
        "  + Nominal\n",
        "  + Ordinal\n",
        "\n",
        "2. Numerical\n",
        "  + Interval\n",
        "  + Ratio"
      ],
      "metadata": {
        "id": "4HIOE0jxkwfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [1] Nominal data\n",
        "\n",
        "+ Sample data description: A research team conducted a study to investigate the relationship between the colors of cars in a parking lot and the satisfaction levels of the owners of those cars.\n",
        "\n",
        "> **Color:** Red, Blue, Green, White, Black, Yellow\n",
        "\n",
        "> **Satisfaction:**\n",
        "> + 1 = Very Dissatisfied\n",
        "> + 2 = Dissatisfied\n",
        "> + 3 = Neutral\n",
        "> + 4 = Satisfied\n",
        "> + 5 = Very Satisfied"
      ],
      "metadata": {
        "id": "ntTjIeKflEPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Generate the data (df1 = data01.csv)\n",
        "import pandas as pd\n",
        "\n",
        "# Create the nominal data DataFrame\n",
        "nominal_data = pd.DataFrame({\n",
        "    'CarID': range(1, 101),\n",
        "    'Color': ['Red', 'Blue', 'Green', 'Red', 'White', 'Blue', 'Green', 'Black', 'White', 'Yellow'] * 10\n",
        "})\n",
        "\n",
        "# Create the ordinal data DataFrame\n",
        "ordinal_data = pd.DataFrame({\n",
        "    'CustomerID': range(1, 101),\n",
        "    'Satisfaction': [4, 3, 5, 2, 4, 3, 4, 1, 5, 2, 4, 3, 4, 2, 5, 4, 2, 3, 5, 1,\n",
        "                     4, 3, 4, 2, 5, 4, 2, 3, 5, 1, 4, 3, 4, 2, 5, 4, 2, 3, 5, 1,\n",
        "                     4, 3, 4, 2, 5, 4, 2, 3, 5, 1, 4, 3, 4, 2, 5, 4, 2, 3, 5, 1,\n",
        "                     4, 3, 4, 2, 5, 4, 2, 3, 5, 1, 4, 3, 4, 2, 5, 4, 2, 3, 5, 1,\n",
        "                     4, 3, 4, 2, 5, 4, 2, 3, 5, 1, 4, 3, 4, 2, 5, 4, 2, 3, 5, 1]\n",
        "})\n",
        "\n",
        "# Combine the two DataFrames on a common key (for example, CarID and CustomerID)\n",
        "combined_data = pd.merge(nominal_data, ordinal_data, left_on='CarID', right_on='CustomerID')\n",
        "\n",
        "# Drop the redundant key (CustomerID)\n",
        "combined_data = combined_data.drop(columns=['CustomerID'])\n",
        "\n",
        "# Save the combined dataset to a CSV file\n",
        "combined_data.to_csv(\"data01.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TZyNorRwm0mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"data01.csv\")\n",
        "\n",
        "df1.tail()"
      ],
      "metadata": {
        "id": "Kws8_QYwoJHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [2] Numeric data\n",
        "\n",
        "+ Sample data description: Collect data on monthly electricity consumption (unit in kWH, ratio data) in households and the number of occupants (interval data). Investigate how household size affects energy usage.\n",
        "\n",
        "> + **Area**: Urban, Rural\n",
        "> + **Electricity**: in kWH\n",
        "> + **Occupants**: integer (ratio data)\n",
        "> + **Daily indoor Temperature**: in Celsius (interval data)\n",
        "\n",
        "+ [Related article](https://www.treehugger.com/urban-or-rural-which-is-more-energy-efficient-4863586)"
      ],
      "metadata": {
        "id": "3o2CC7HAlHm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Data to generate (df2= data02.csv)\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Create a list of areas (50 city and 50 rural)\n",
        "areas = [\"Urban\"] * 50 + [\"Rural\"] * 50\n",
        "\n",
        "# Generate random occupants data\n",
        "occupants = [random.randint(1, 5) for _ in range(100)]  # Random values between 1 and 5 occupants\n",
        "\n",
        "# Generate electricity consumption data with a tendency for rural areas to use more electricity\n",
        "electricity = []\n",
        "\n",
        "for area, occupant in zip(areas, occupants):\n",
        "    if area == \"Urban\":\n",
        "        # Generate electricity consumption for the city (lower range)\n",
        "        consumption = random.uniform(200, 400) + 50 * occupant  # Random values between 200 and 400 kWh, with occupancy effect\n",
        "    else:\n",
        "        # Generate electricity consumption for rural areas (higher range)\n",
        "        consumption = random.uniform(300, 600) + 75 * occupant  # Random values between 300 and 600 kWh, with occupancy effect\n",
        "\n",
        "    electricity.append(consumption)\n",
        "\n",
        "# Generate daily temperature data in Celsius with a positive correlation to occupants\n",
        "daily_temperature = [20 + 1.5 * occupant + random.uniform(-2, 2) for occupant in occupants]\n",
        "daily_temperature_rounded = [round(temp, 1) for temp in daily_temperature]\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame({'Area': areas, 'Electricity': electricity, 'Occupants': occupants, 'Daily Temperature (Â°C)': daily_temperature_rounded})\n",
        "\n",
        "# Save the DataFrame to 'data02.csv' file\n",
        "data.to_csv('data02.csv', index=False)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "o39xqVPOtfnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('/content/data02.csv')\n",
        "df2"
      ],
      "metadata": {
        "id": "z4OjfZCQthPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# ðŸŒ€ **2. Descriptive statistics**\n",
        "\n",
        "Summarizing the data\n"
      ],
      "metadata": {
        "id": "MbefBB1r9eSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [1] Descriptive stat for categorical data\n",
        "> ðŸ”µ data.describe() # This is for numerical data\n",
        "\n",
        "Currently, data = df1, df2"
      ],
      "metadata": {
        "id": "ObR5k8sLpKk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.describe()"
      ],
      "metadata": {
        "id": "s_8ZxN5ipJ6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.describe()"
      ],
      "metadata": {
        "id": "f1eOUIHSAK7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For categorical data: Count data\n",
        "\n",
        "> ðŸ”µ variable = df['Color'].value_counts() # This is for count data"
      ],
      "metadata": {
        "id": "MH494KRvpotU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each color\n",
        "color_counts = df1['Color'].value_counts()\n",
        "color_counts"
      ],
      "metadata": {
        "id": "VEhxpGjCrhES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Note:_ 'int64': This indicates that the data in the column consists of 64-bit integers"
      ],
      "metadata": {
        "id": "kgO4JhhaqPTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each satisfaction level\n",
        "satisfaction_counts = df1['Satisfaction'].value_counts()\n",
        "print(satisfaction_counts)"
      ],
      "metadata": {
        "id": "QfO0DgBtpuke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [2] Descriptive stat for Numerical data\n",
        "\n"
      ],
      "metadata": {
        "id": "ORicOZmlk4a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.describe()"
      ],
      "metadata": {
        "id": "C1UrcXv4ue0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e.g., Describe by Area\n",
        "\n",
        "> data.groupby('Area').describe()"
      ],
      "metadata": {
        "id": "Hk4dY0Lo_TkO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wQF-JJwj1oK"
      },
      "outputs": [],
      "source": [
        "summary_by_area = df2.groupby('Area').describe()\n",
        "\n",
        "# Display the summary statistics\n",
        "print(summary_by_area)\n"
      ]
    }
  ]
}